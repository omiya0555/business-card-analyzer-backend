from __future__ import annotations

import os
import re
import uuid
import logging
from pathlib import Path
from typing import List, Sequence, Dict
from tqdm import tqdm
from dotenv import load_dotenv

import openai
from pinecone.grpc import PineconeGRPC as Pinecone

# ---------------------------------------------------------------------------- #
# Configuration & logging
# ---------------------------------------------------------------------------- #
logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
LOGGER = logging.getLogger(__name__)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
BOOTH_DIR = "booth"
TOPICS_DIR = "topics"

def load_env():
    load_dotenv()
    return {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "pinecone_api_key": os.getenv("PINECONE_API_KEY"),
        "pinecone_booth_index_name": os.getenv("PINECONE_BOOTH_INDEX_NAME", "booth-chunks"),
        "pinecone_topic_index_name": os.getenv("PINECONE_TOPIC_INDEX_NAME", "topic-chunks"),
    }

# ---------------------------------------------------------------------------- #
# Helpers
# ---------------------------------------------------------------------------- #

def init_openai(api_key: str):
    openai.api_key = api_key
    return openai.OpenAI(api_key=api_key)

def init_pinecone(api_key: str, index_name: str):
    pc = Pinecone(api_key=api_key)
    return pc.Index(index_name)

def markdown_to_plain(md_text: str) -> str:
    """Markdown ã‚’ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
    # Markdown ã®ç°¡æ˜“å¤‰æ› (HTMLã‚¿ã‚°ã‚’å‰Šé™¤)
    return re.sub(r"<[^>]+>", "", md_text).replace("\n", " ").strip()

def embed_text(client: openai.OpenAI, text: str) -> List[float]:
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=[text]
    )
    return response.data[0].embedding

def chunk_text(text: str, max_chars: int) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ãŸæ–‡å­—æ•°ã§ãƒãƒ£ãƒ³ã‚¯åŒ–"""
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]

def process_md(file_path: str, file_name: str, client: openai.OpenAI, data_type: str) -> List[Dict]:
    """Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Šã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    with open(file_path, "r", encoding="utf-8") as f:
        md_text = f.read()
    plain_text = markdown_to_plain(md_text)
    chunks = chunk_text(plain_text, max_chars=500)  # 500æ–‡å­—ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
    vectors = []
    for idx, chunk in enumerate(chunks):
        embedding = embed_text(client, chunk)
        vectors.append({
            "id": f"{data_type}-{file_name}-chunk{idx+1}",
            "values": embedding,
            "metadata": {
                "text": chunk,
                "source": file_name,
                "chunk_index": idx + 1,
                "data_type": data_type
            }
        })
    return vectors

def process_directory(directory: str, data_type: str, client: openai.OpenAI) -> List[Dict]:
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    if not os.path.exists(directory):
        LOGGER.warning(f"Directory '{directory}' does not exist. Skipping...")
        return []
    
    all_vectors = []
    for fname in tqdm(os.listdir(directory), desc=f"ğŸ“„ Processing {data_type} files"):
        if not fname.lower().endswith(".md"):
            continue
        path = os.path.join(directory, fname)
        vectors = process_md(path, fname, client, data_type)
        all_vectors.extend(vectors)
    
    return all_vectors

# ---------------------------------------------------------------------------- #
# Main upsert routine
# ---------------------------------------------------------------------------- #

def upsert_topics():
    """ä¸¡æ–¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    env = load_env()
    client = init_openai(env["openai_api_key"])
    
    # Booth ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‡¦ç†
    booth_index = init_pinecone(env["pinecone_api_key"], env["pinecone_booth_index_name"])
    booth_vectors = process_directory(BOOTH_DIR, "booth", client)
    
    if booth_vectors:
        print(f"\nâœ¨ Booth ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã« {len(booth_vectors)} ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        booth_index.upsert(booth_vectors)
        print("âœ… Booth ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
    else:
        print("âš ï¸  Booth ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # Topic ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‡¦ç†
    topic_index = init_pinecone(env["pinecone_api_key"], env["pinecone_topic_index_name"])
    topic_vectors = process_directory(TOPICS_DIR, "tpics", client)
    
    if topic_vectors:
        print(f"\nâœ¨ Topic ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã« {len(topic_vectors)} ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        topic_index.upsert(topic_vectors)
        print("âœ… Topic ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
    else:
        print("âš ï¸  Topic ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    print(f"\nğŸ‰ å…¨ä½“ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"   - Booth vectors: {len(booth_vectors)}")
    print(f"   - Topic vectors: {len(topic_vectors)}")

if __name__ == "__main__":
    upsert_topics()
